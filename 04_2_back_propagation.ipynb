{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4장 보강자료\n",
    "\n",
    "간단한 데이터로 살펴보는 Backpropagation 예시 및 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. 데이터 및 변수 초기화\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # XOR 연산의 입력 값\n",
    "y = np.array([[0], [1], [1], [0]])             # XOR 연산의 결과 값\n",
    "\n",
    "# 가중치와 바이어스 초기화\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요한 함수들을 정의합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 구조\n",
    "\n",
    "1-레이어이고, input 변수가 2개인 신경망을 사용해서 backpropagation을 구현해봅시다.  \n",
    "히든레이어의 뉴런의 수는 2개 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "\n",
    "\n",
    "$$\n",
    "z_1 = XW_1 + b_1\n",
    "$$\n",
    "$$\n",
    "a_1 = \\sigma(z_1)\n",
    "$$\n",
    "$$\n",
    "L = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - a1_i)^2\n",
    "$$\n",
    "\n",
    "### Backward Pass\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a1} = (a1 - y)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z1} = \\frac{\\partial L}{\\partial a1} \\times \\sigma'(a_1)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W1} = X^T \\frac{\\partial L}{\\partial z1}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b1} = \\sum \\frac{\\partial L}{\\partial z1}\n",
    "$$\n",
    "\n",
    "### Weight Update:\n",
    "\n",
    "$$\n",
    "W_1 = W_1 - \\text{learning\\_rate} \\times \\frac{\\partial L}{\\partial W1}\n",
    "$$\n",
    "$$\n",
    "b_1 = b_1 - \\text{learning\\_rate} \\times \\frac{\\partial L}{\\partial b1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10000, Loss: 0.2563\n",
      "Epoch 1000/10000, Loss: 0.2500\n",
      "Epoch 2000/10000, Loss: 0.2500\n",
      "Epoch 3000/10000, Loss: 0.2500\n",
      "Epoch 4000/10000, Loss: 0.2500\n",
      "Epoch 5000/10000, Loss: 0.2500\n",
      "Epoch 6000/10000, Loss: 0.2500\n",
      "Epoch 7000/10000, Loss: 0.2500\n",
      "Epoch 8000/10000, Loss: 0.2500\n",
      "Epoch 9000/10000, Loss: 0.2500\n",
      "Predicted Output:\n",
      "[[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n"
     ]
    }
   ],
   "source": [
    "input_size = 2\n",
    "output_size = 1\n",
    "\n",
    "W1 = np.random.randn(input_size, output_size)\n",
    "b1 = np.zeros((1, output_size))\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "# 3. 학습 시작\n",
    "for epoch in range(epochs):\n",
    "    # Forward Pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    # 4. 손실 계산\n",
    "    loss = np.mean((y - a1) ** 2)\n",
    "\n",
    "    # 5. Backward Pass\n",
    "    da1 = (a1 - y)\n",
    "    dz1 = da1 * sigmoid_derivative(a1)\n",
    "    dW1 = np.dot(X.T, dz1)\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "    # 6. 가중치 업데이트\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "    # 일정 에포크마다 손실 출력\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "# 최종 예측 출력\n",
    "print(\"Predicted Output:\")\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네, 간단한 2-레이어 신경망을 사용하여 backpropagation을 구현함. \n",
    "\n",
    "이 예제에서는:\n",
    "\n",
    "1. 입력 레이어와 2개의 숨겨진 레이어를 가진 신경망을 구축합니다.\n",
    "2. 시그모이드를 활성화 함수로 사용합니다.\n",
    "3. 평균 제곱 오차를 손실 함수로 사용합니다.\n",
    "\n",
    "필요한 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model:\n",
    "\n",
    "#### 1. Forward Pass:\n",
    "1. **Hidden Layer**:\n",
    "    - $$ z_1 = X \\times W_1 + b_1 $$\n",
    "    - $$ a_1 = \\sigma(z_1) $$\n",
    "\n",
    "2. **Output Layer**:\n",
    "    - $$ z_2 = a_1 \\times W_2 + b_2 $$\n",
    "    - $$ a_2 = \\sigma(z_2) $$\n",
    "\n",
    "여기서,   $ \\sigma $ 는 sigmoid 활성화 함수를 의미합니다.\n",
    "\n",
    "#### 2. Loss Function:\n",
    "- $$ L = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - a_{2,i})^2 $$\n",
    "\n",
    "여기서, $ N $ 은 데이터의 개수를 의미합니다.\n",
    "\n",
    "#### 3. Backward Pass (Gradients):\n",
    "\n",
    "- **Output Layer**:\n",
    "    - $$ \\frac{\\partial L}{\\partial a_2} = 2(a_2 - y) $$\n",
    "    - $$ \\frac{\\partial L}{\\partial z_2} = \\frac{\\partial L}{\\partial a_2} \\times \\sigma'(z_2) $$\n",
    "    - $$ \\frac{\\partial L}{\\partial W_2} = a_1^T \\times \\frac{\\partial L}{\\partial z_2} $$\n",
    "    - $$ \\frac{\\partial L}{\\partial b_2} = \\sum_{i=1}^{N} \\frac{\\partial L}{\\partial z_{2,i}} $$\n",
    "\n",
    "- **Hidden Layer**:\n",
    "    - $$ \\frac{\\partial L}{\\partial a_1} = \\frac{\\partial L}{\\partial z_2} \\times W_2^T $$\n",
    "    - $$ \\frac{\\partial L}{\\partial z_1} = \\frac{\\partial L}{\\partial a_1} \\times \\sigma'(z_1) $$\n",
    "    - $$ \\frac{\\partial L}{\\partial W_1} = X^T \\times \\frac{\\partial L}{\\partial z_1} $$\n",
    "    - $$ \\frac{\\partial L}{\\partial b_1} = \\sum_{i=1}^{N} \\frac{\\partial L}{\\partial z_{1,i}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 2-레이어 신경망의 파라미터를 초기화하겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.64768854,  1.52302986, -0.23415337],\n",
       "       [-0.23413696,  1.57921282,  0.76743473]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 실제로 forward pass 및 backpropagation을 실행할 학습 코드를 작성하겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10000, Loss: 0.2440\n",
      "Epoch 1000/10000, Loss: 0.1881\n",
      "Epoch 2000/10000, Loss: 0.0920\n",
      "Epoch 3000/10000, Loss: 0.0313\n",
      "Epoch 4000/10000, Loss: 0.0149\n",
      "Epoch 5000/10000, Loss: 0.0092\n",
      "Epoch 6000/10000, Loss: 0.0065\n",
      "Epoch 7000/10000, Loss: 0.0049\n",
      "Epoch 8000/10000, Loss: 0.0040\n",
      "Epoch 9000/10000, Loss: 0.0033\n",
      "Predicted Output:\n",
      "[[0.02224667]\n",
      " [0.94486905]\n",
      " [0.94514992]\n",
      " [0.06836053]]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward Pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # 손실 계산\n",
    "    loss = np.mean((y - a2) ** 2)\n",
    "\n",
    "    # Backward Pass\n",
    "    da2 = (a2 - y)\n",
    "    dz2 = da2 * sigmoid_derivative(a2)\n",
    "    dW2 = np.dot(a1.T, dz2)\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "    da1 = np.dot(dz2, W2.T)\n",
    "    dz1 = da1 * sigmoid_derivative(a1)\n",
    "    dW1 = np.dot(X.T, dz1)\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "    # 가중치 업데이트\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "    # 일정 에포크마다 손실 출력\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "# 최종 예측 출력\n",
    "print(\"Predicted Output:\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "히든 레이어의 수가 2개이고, 각 레이어의 뉴런의 수가 3개인 뉴럴네트워크를 같은 형태로 구현해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled A: [0.   0.25 0.5  0.75 1.  ]\n",
      "Scaled B: [0.   0.25 0.5  0.75 1.  ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def min_max_scaling(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "A = np.array([10, 20, 30, 40, 50])\n",
    "B = np.array([500, 600, 700, 800, 900])\n",
    "\n",
    "A_scaled = min_max_scaling(A)\n",
    "B_scaled = min_max_scaling(B)\n",
    "\n",
    "print(\"Scaled A:\", A_scaled)\n",
    "print(\"Scaled B:\", B_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized A: [-1.41421356 -0.70710678  0.          0.70710678  1.41421356]\n",
      "Standardized B: [-1.49120227 -0.67186036  0.14748154  0.63908669  1.3764944 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def standardization(data):\n",
    "    return (data - np.mean(data)) / np.std(data)\n",
    "\n",
    "A = np.array([10, 20, 30, 40, 50])\n",
    "B = np.array([500, 600, 700, 760, 850])\n",
    "\n",
    "A_standard = standardization(A)\n",
    "B_standard = standardization(B)\n",
    "\n",
    "print(\"Standardized A:\", A_standard)\n",
    "print(\"Standardized B:\", B_standard)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
