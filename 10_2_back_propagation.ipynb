{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4장 보강자료\n",
    "\n",
    "간단한 데이터로 살펴보는 Backpropagation 예시 및 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. 데이터 및 변수 초기화\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # XOR 연산의 입력 값\n",
    "y = np.array([[0], [1], [1], [0]])             # XOR 연산의 결과 값\n",
    "\n",
    "# 가중치와 바이어스 초기화\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요한 함수들을 정의합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 구조\n",
    "\n",
    "1-레이어이고, input 변수가 2개인 신경망을 사용해서 backpropagation을 구현해봅시다.  \n",
    "히든레이어의 뉴런의 수는 2개 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "\n",
    "\n",
    "$$\n",
    "z_1 = XW_1 + b_1\n",
    "$$\n",
    "$$\n",
    "a_1 = \\sigma(z_1)\n",
    "$$\n",
    "$$\n",
    "L = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - a_{1,i})^2\n",
    "$$\n",
    "\n",
    "### Backward Pass\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a_1} = (a_1 - y)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_1} = \\frac{\\partial L}{\\partial a_1} \\times \\sigma'(a_1)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_1} = X^T \\frac{\\partial L}{\\partial z_1}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_1} = \\sum \\frac{\\partial L}{\\partial z_1}\n",
    "$$\n",
    "\n",
    "### Weight Update:\n",
    "\n",
    "$$\n",
    "W_1 = W_1 - \\text{learning\\_rate} \\times \\frac{\\partial L}{\\partial W_1}\n",
    "$$\n",
    "$$\n",
    "b_1 = b_1 - \\text{learning\\_rate} \\times \\frac{\\partial L}{\\partial b_1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10000, Loss: 0.2563\n",
      "W1: [ 0.49135794 -0.13921903], b1: [[-0.0045571]]\n",
      "Epoch 1000/10000, Loss: 0.2500\n",
      "W1: [0.00273374 0.00150596], b1: [[-0.00251337]]\n",
      "Epoch 2000/10000, Loss: 0.2500\n",
      "W1: [4.27351493e-05 4.04095738e-05], b1: [[-4.93102057e-05]]\n",
      "Epoch 3000/10000, Loss: 0.2500\n",
      "W1: [8.18041359e-07 8.13639082e-07], b1: [[-9.67701081e-07]]\n",
      "Epoch 4000/10000, Loss: 0.2500\n",
      "W1: [1.60149174e-08 1.60065840e-08], b1: [[-1.89910024e-08]]\n",
      "Epoch 5000/10000, Loss: 0.2500\n",
      "W1: [3.14216711e-10 3.14200937e-10], b1: [[-3.72695867e-10]]\n",
      "Epoch 6000/10000, Loss: 0.2500\n",
      "W1: [6.16628951e-12 6.16626961e-12], b1: [[-7.31408398e-12]]\n",
      "Epoch 7000/10000, Loss: 0.2500\n",
      "W1: [1.20978037e-13 1.20977565e-13], b1: [[-1.43525393e-13]]\n",
      "Epoch 8000/10000, Loss: 0.2500\n",
      "W1: [2.35625830e-15 2.35578642e-15], b1: [[-2.83238003e-15]]\n",
      "Epoch 9000/10000, Loss: 0.2500\n",
      "W1: [2.46834548e-16 2.46362677e-16], b1: [[-3.31602665e-16]]\n",
      "Predicted Output:\n",
      "[[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n"
     ]
    }
   ],
   "source": [
    "input_size = 2\n",
    "output_size = 1\n",
    "\n",
    "W1 = np.random.randn(input_size, output_size)\n",
    "b1 = np.zeros((1, output_size))\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 10000 #10000번의 작업\n",
    "\n",
    "# 3. 학습 시작\n",
    "for epoch in range(epochs):\n",
    "    # Forward Pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    # 4. 손실 계산\n",
    "    loss = np.mean((y - a1) ** 2)\n",
    "\n",
    "    # 5. Backward Pass\n",
    "    da1 = (a1 - y)\n",
    "    dz1 = da1 * sigmoid_derivative(a1)\n",
    "    dW1 = np.dot(X.T, dz1)\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "    # 6. 가중치 업데이트\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "    # 일정 에포크마다 손실 출력\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss:.4f}\")\n",
    "        print(f\"W1: {W1.flatten()}, b1: {b1}\")\n",
    "\n",
    "# 최종 예측 출력\n",
    "print(\"Predicted Output:\")\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.46834548e-16],\n",
       "       [2.46362677e-16]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.46834548e-16, 2.46362677e-16])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네, 간단한 2-레이어 신경망을 사용하여 backpropagation을 구현함. \n",
    "\n",
    "이 예제에서는:\n",
    "\n",
    "1. 입력 레이어와 2개의 숨겨진 레이어를 가진 신경망을 구축합니다.\n",
    "2. 시그모이드를 활성화 함수로 사용합니다.\n",
    "3. 평균 제곱 오차를 손실 함수로 사용합니다.\n",
    "\n",
    "필요한 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model:\n",
    "\n",
    "#### 1. Forward Pass:\n",
    "1. **Hidden Layer**:\n",
    "    - $$ z_1 = X \\times W_1 + b_1 $$\n",
    "    - $$ a_1 = \\sigma(z_1) $$\n",
    "\n",
    "2. **Output Layer**:\n",
    "    - $$ z_2 = a_1 \\times W_2 + b_2 $$\n",
    "    - $$ a_2 = \\sigma(z_2) $$\n",
    "\n",
    "여기서,   $ \\sigma $ 는 sigmoid 활성화 함수를 의미합니다.\n",
    "\n",
    "#### 2. Loss Function:\n",
    "- $$ L = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - a_{2,i})^2 $$\n",
    "\n",
    "여기서, $ N $ 은 데이터의 개수를 의미합니다.\n",
    "\n",
    "#### 3. Backward Pass (Gradients):\n",
    "\n",
    "- **Output Layer**:\n",
    "    - $$ \\frac{\\partial L}{\\partial a_2} = 2(a_2 - y) $$\n",
    "    - $$ \\frac{\\partial L}{\\partial z_2} = \\frac{\\partial L}{\\partial a_2} \\times \\sigma'(z_2) $$\n",
    "    - $$ \\frac{\\partial L}{\\partial W_2} = a_1^T \\times \\frac{\\partial L}{\\partial z_2} $$\n",
    "    - $$ \\frac{\\partial L}{\\partial b_2} = \\sum_{i=1}^{N} \\frac{\\partial L}{\\partial z_{2,i}} $$\n",
    "\n",
    "- **Hidden Layer**:\n",
    "    - $$ \\frac{\\partial L}{\\partial a_1} = \\frac{\\partial L}{\\partial z_2} \\times W_2^T $$\n",
    "    - $$ \\frac{\\partial L}{\\partial z_1} = \\frac{\\partial L}{\\partial a_1} \\times \\sigma'(z_1) $$\n",
    "    - $$ \\frac{\\partial L}{\\partial W_1} = X^T \\times \\frac{\\partial L}{\\partial z_1} $$\n",
    "    - $$ \\frac{\\partial L}{\\partial b_1} = \\sum_{i=1}^{N} \\frac{\\partial L}{\\partial z_{1,i}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 2-레이어 신경망의 파라미터를 초기화하겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.64768854,  1.52302986, -0.23415337],\n",
       "       [-0.23413696,  1.57921282,  0.76743473]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 실제로 forward pass 및 backpropagation을 실행할 학습 코드를 작성하겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10000, Loss: 0.2440\n",
      "Epoch 1000/10000, Loss: 0.1881\n",
      "Epoch 2000/10000, Loss: 0.0920\n",
      "Epoch 3000/10000, Loss: 0.0313\n",
      "Epoch 4000/10000, Loss: 0.0149\n",
      "Epoch 5000/10000, Loss: 0.0092\n",
      "Epoch 6000/10000, Loss: 0.0065\n",
      "Epoch 7000/10000, Loss: 0.0049\n",
      "Epoch 8000/10000, Loss: 0.0040\n",
      "Epoch 9000/10000, Loss: 0.0033\n",
      "Predicted Output:\n",
      "[[0.02224667]\n",
      " [0.94486905]\n",
      " [0.94514992]\n",
      " [0.06836053]]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward Pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # 손실 계산\n",
    "    loss = np.mean((y - a2) ** 2)\n",
    "\n",
    "    # Backward Pass\n",
    "    da2 = (a2 - y)\n",
    "    dz2 = da2 * sigmoid_derivative(a2)\n",
    "    dW2 = np.dot(a1.T, dz2)\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "    da1 = np.dot(dz2, W2.T)\n",
    "    dz1 = da1 * sigmoid_derivative(a1)\n",
    "    dW1 = np.dot(X.T, dz1)\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "    # 가중치 업데이트\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "    # 일정 에포크마다 손실 출력\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "# 최종 예측 출력\n",
    "print(\"Predicted Output:\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.78947689,  5.61073991, -2.66391388],\n",
       "       [-2.47555936,  5.6326184 ,  5.82295068]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "히든 레이어의 수가 2개이고, 각 레이어의 뉴런의 수가 3개인 뉴럴네트워크를 같은 형태로 구현해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10000, Loss: 0.2892\n",
      "Epoch 1000/10000, Loss: 0.2492\n",
      "Epoch 2000/10000, Loss: 0.2442\n",
      "Epoch 3000/10000, Loss: 0.2191\n",
      "Epoch 4000/10000, Loss: 0.1803\n",
      "Epoch 5000/10000, Loss: 0.1519\n",
      "Epoch 6000/10000, Loss: 0.1029\n",
      "Epoch 7000/10000, Loss: 0.0123\n",
      "Epoch 8000/10000, Loss: 0.0047\n",
      "Epoch 9000/10000, Loss: 0.0027\n",
      "Predicted Output:\n",
      "[[0.0281708 ]\n",
      " [0.95702334]\n",
      " [0.95697581]\n",
      " [0.05356113]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 입력 특성의 수\n",
    "input_size = 2\n",
    "\n",
    "# 각 히든 레이어의 뉴런 수\n",
    "hidden_layer1_size = 3\n",
    "hidden_layer2_size = 3\n",
    "\n",
    "# 출력 뉴런의 수\n",
    "output_size = 1\n",
    "\n",
    "# 학습 속도\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 학습 횟수\n",
    "epochs = 10000\n",
    "\n",
    "# 시그모이드 활성화 함수\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# 시그모이드 함수의 도함수\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# 입력 데이터와 정답 데이터 (임의의 값)\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# 1번째 히든 레이어의 가중치와 편향 초기화\n",
    "W1 = np.random.randn(input_size, hidden_layer1_size)\n",
    "b1 = np.zeros((1, hidden_layer1_size))\n",
    "\n",
    "# 2번째 히든 레이어의 가중치와 편향 초기화\n",
    "W2 = np.random.randn(hidden_layer1_size, hidden_layer2_size)\n",
    "b2 = np.zeros((1, hidden_layer2_size))\n",
    "\n",
    "# 출력 레이어의 가중치와 편향 초기화\n",
    "W3 = np.random.randn(hidden_layer2_size, output_size)\n",
    "b3 = np.zeros((1, output_size))\n",
    "\n",
    "# 학습 시작\n",
    "for epoch in range(epochs):\n",
    "    # Forward Pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    z3 = np.dot(a2, W3) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    # 손실 계산\n",
    "    loss = np.mean((y - a3) ** 2)\n",
    "\n",
    "    # Backward Pass\n",
    "    da3 = (a3 - y) * sigmoid_derivative(a3)\n",
    "    dz3 = da3\n",
    "    dW3 = np.dot(a2.T, dz3)\n",
    "    db3 = np.sum(dz3, axis=0, keepdims=True)\n",
    "\n",
    "    da2 = np.dot(dz3, W3.T) * sigmoid_derivative(a2)\n",
    "    dz2 = da2\n",
    "    dW2 = np.dot(a1.T, dz2)\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "    da1 = np.dot(dz2, W2.T) * sigmoid_derivative(a1)\n",
    "    dz1 = da1\n",
    "    dW1 = np.dot(X.T, dz1)\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "    # 가중치 업데이트\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "    W3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * db3\n",
    "\n",
    "    # 일정 에포크마다 손실 출력\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "# 최종 예측 출력\n",
    "print(\"Predicted Output:\")\n",
    "print(a3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
